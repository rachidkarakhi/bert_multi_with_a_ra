{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgKJkPUxvPSb",
        "outputId": "91e19c54-926f-4900-b3eb-179784791db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: textattack in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.3.13)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.10/dist-packages (from textattack) (0.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.13.1)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.10/dist-packages (from textattack) (2.7.1)\n",
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.10/dist-packages (from textattack) (0.2.3)\n",
            "Requirement already satisfied: lru-dict in /usr/local/lib/python3.10/dist-packages (from textattack) (1.3.0)\n",
            "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.11.3)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.35.2)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (from textattack) (3.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.1)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from textattack) (1.1)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from textattack) (0.5.13)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n",
            "Requirement already satisfied: pinyin>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.4.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
            "Requirement already satisfied: OpenHowNet in /usr/local/lib/python3.10/dist-packages (from textattack) (2.0)\n",
            "Requirement already satisfied: pycld2 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.41)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (23.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.3.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.0)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.29.6)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.3.4)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.14)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (6.1.3)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.6.6)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.2)\n",
            "Requirement already satisfied: janome>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.5.0)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.3)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.5.9)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.1)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.3.2)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.26.18)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.6.0)\n",
            "Requirement already satisfied: semver<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->textattack) (0.6.2)\n",
            "Requirement already satisfied: anytree in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (2.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n",
            "Requirement already satisfied: botocore<1.33.0,>=1.32.6 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.32.6)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair->textattack) (0.1.99)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack) (6.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (3.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.24.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers>=4.30.0->textattack) (5.9.5)\n",
            "GPU is available:  []\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install textattack\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import random\n",
        "import pandas as pd\n",
        "from nltk.corpus import wordnet\n",
        "print(\"GPU is available: \", tf.config.experimental.list_physical_devices(\"GPU\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8moigzkvhgg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrNEYuCC0gFX"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# # Initialize the translation pipeline\n",
        "# translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "# # Original text\n",
        "# original_text = \"Since the windshield exerts a force on the mosquito, which we can call action, the mosquito exerts an equal and opposite force on the windshield, called the reaction.\"\n",
        "\n",
        "# # Translate to French\n",
        "# translated_text = translator(original_text, max_length=400)[0]['translation_text']\n",
        "\n",
        "# # Translate back to English\n",
        "# back_translated_text = translator(translated_text, max_length=40)[0]['translation_text']\n",
        "\n",
        "# # Collect augmented data\n",
        "# augmented_data = [original_text, back_translated_text]\n",
        "\n",
        "# # You can add these to your dataset for data augmentation\n",
        "# print(augmented_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK5Q-iocvPSe"
      },
      "source": [
        "Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PielER8u2UFU"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "def synonym_replacement(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        synsets = wordnet.synsets(word)\n",
        "        if synsets:\n",
        "            synonyms = [lemma.name() for synset in synsets for lemma in synset.lemmas()]\n",
        "            if synonyms:\n",
        "                new_word = random.choice(synonyms)\n",
        "                new_words.append(new_word)\n",
        "            else:\n",
        "                new_words.append(word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "original_text = \"Since the windshield exerts a force on the mosquito, which we can call action, the mosquito exerts an equal and opposite force on the windshield, called the reaction..\"\n",
        "augmented_text = synonym_replacement(original_text)\n",
        "print(augmented_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_rnsdJKvPSg"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "def get_data_from_xml_child(child):\n",
        "    Row = {}\n",
        "    Row[\"StudentID\"] = child.find(\"MetaInfo\").get('StudentID')\n",
        "    Row[\"TaskID\"] = child.find(\"MetaInfo\").get('TaskID')\n",
        "    Row[\"DataSource\"] = child.find(\"MetaInfo\").get('DataSource')\n",
        "    Row[\"ProblemDescription\"] = child.find(\"ProblemDescription\").text\n",
        "    Row[\"Question\"] = child.find(\"Question\").text\n",
        "    Row[\"Answer\"] = child.find(\"Answer\").text\n",
        "    Row[\"Annotation_label\"] = child.find(\"Annotation\").get(\"Label\")\n",
        "    Row[\"Annotation_ContextRequired\"] = child.find(\"Annotation\").find(\"AdditionalAnnotation\").get('ContextRequired')\n",
        "    Row[\"Annotation_ExtraInfoInAnswer\"] = child.find(\"Annotation\").find(\"AdditionalAnnotation\").get('ExtraInfoInAnswer')\n",
        "    Row[\"Annotation_comments\"] = child.find(\"Annotation\").find(\"Comments\").text\n",
        "    Row[\"Annotation_comments_watch\"] = child.find(\"Annotation\").find(\"Comments\").get(\"Watch\")\n",
        "    Row[\"ReferenceAnswers\"] = child.find(\"ReferenceAnswers\").text\n",
        "    return Row\n",
        "\n",
        "# xml_data = open('/kaggle/input/asag-dt-grad-xml/grade_data.xml', 'r').read()  # Read file\n",
        "# Read the file from your Google Drive\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/grade_data.xml', 'r') as f:\n",
        "    xml_data = f.read()\n",
        "root = ET.XML(xml_data)  # Parse XML\n",
        "\n",
        "data = []\n",
        "cols = []\n",
        "for i, child in enumerate(root):\n",
        "    data.append(get_data_from_xml_child(child))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLd4yP4svPSh"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h37we9TEvPSk"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_YfiqqZvPSl"
      },
      "outputs": [],
      "source": [
        "df_draft = df[['ProblemDescription', 'Question',\n",
        "       'Answer', 'Annotation_ContextRequired', 'Annotation_ExtraInfoInAnswer',\n",
        "       'Annotation_comments', 'Annotation_comments_watch', 'ReferenceAnswers',\"Annotation_label\"]]\n",
        "df_draft.columns = ['PD', 'Q','A', 'ACR', 'AEI','AC', 'ACW', 'RA',\"target\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UxpBFYlvPSm"
      },
      "outputs": [],
      "source": [
        "df_draft.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SXU0RmZvPSn"
      },
      "outputs": [],
      "source": [
        "def change_label(txt):\n",
        "    rs = \"\"\n",
        "    splits = str(txt).split(\"|\")\n",
        "\n",
        "\n",
        "    for sp in splits:\n",
        "        #print(sp)\n",
        "        if \"1\" in sp:\n",
        "            #print(\"in\")\n",
        "            rs = rs+sp+\" \"\n",
        "\n",
        "    return rs.replace(\"(1)\",\"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOOvfW0CvPSo"
      },
      "outputs": [],
      "source": [
        "df_draft[\"target\"] = df_draft[\"target\"].apply(change_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZf4ujdovPSo"
      },
      "outputs": [],
      "source": [
        "df_draft[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQPtGg9V7W7r"
      },
      "outputs": [],
      "source": [
        "df1 = df_draft[[\"A\",\"RA\",\"target\",\"ACR\",\"AEI\",\"ACW\"]]\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZSFMw3EvPSp"
      },
      "outputs": [],
      "source": [
        "#  df_draft[\"text\"] = df_draft[\"A\"]+\" \"+df_draft[\"RA\"]\n",
        "# df_draft[\"text\"] = df_draft[\"A\"]+\" \"+df_draft[\"RA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHlHUFcSvPSp"
      },
      "outputs": [],
      "source": [
        "# df_draft[\"text\"] = df_draft[\"Q\"]+\" \"+df_darft[\"A\"]+\" \"+df_darft[\"RA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PKwq-F5vPSp"
      },
      "outputs": [],
      "source": [
        "#$ df1 = df_draft[[\"text\",\"target\",\"ACR\",\"AEI\",\"ACW\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ-vuiSwvPSq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dGAjsbavPSq"
      },
      "outputs": [],
      "source": [
        "# len(df1[\"text\"][55])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roy1CM1YvPSq"
      },
      "outputs": [],
      "source": [
        "def transfrom_class(txt):\n",
        "    txt = txt.strip()\n",
        "    if \"correct\" == txt:\n",
        "        return 0\n",
        "    elif \"incorrect\" == txt:\n",
        "        return 1\n",
        "    elif \"correct_but_incomplete\" == txt:\n",
        "        return 2\n",
        "    elif \"contradictory\" == txt:\n",
        "        return 3\n",
        "    elif \"correct_but_incomplete incorrect\" == txt:\n",
        "        return 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RTRlKrUvPSq"
      },
      "outputs": [],
      "source": [
        "df1[\"target\"].apply(transfrom_class).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wj86wTivPSr"
      },
      "outputs": [],
      "source": [
        "df1[\"target\"] = df1[\"target\"].apply(transfrom_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6RkKyzRvPSr"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHJoce4uvPSr"
      },
      "outputs": [],
      "source": [
        "df1 = df1.drop(df1[df1[\"target\"] == 4].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeyGhPA9vPSr"
      },
      "outputs": [],
      "source": [
        "df1[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJqYDLUQvPSs"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWZTsVOEvPSs"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "stopwords = _stop_words.ENGLISH_STOP_WORDS\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean(doc):\n",
        "    #text_no_namedentities = []\n",
        "    #document = nlp(doc)\n",
        "    #ents = [e.text for e in document.ents]\n",
        "    #for item in document:\n",
        "    #    if item.text in ents:\n",
        "    #        pass\n",
        "    #    else:\n",
        "    #        text_no_namedentities.append(item.text)\n",
        "\n",
        "    #doc = (\" \".join(text_no_namedentities))\n",
        "\n",
        "    doc = doc.lower().strip()\n",
        "    doc = doc.replace(\"</br>\", \" \")\n",
        "    doc = doc.replace(\"-\", \" \")\n",
        "\n",
        "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
        "    doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
        "    doc = \"\".join([lemmatizer.lemmatize(word) for word in doc])\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eLs5l17vPSt"
      },
      "outputs": [],
      "source": [
        "df1.A.iloc[0]\n",
        "df1.RA.iloc[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io1cB7navPSt"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "clean(df1.A.iloc[0])\n",
        "clean(df1.RA.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVxprOPZvPSt"
      },
      "outputs": [],
      "source": [
        "df1[\"A\"] = df1.A.apply(clean)\n",
        "df1[\"RA\"] = df1.RA.apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3JBnrHrvPSt"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBuBRFRRvPSu"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_csv(\"/kaggle/input/asag-dt-grad/clean_data_4_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP56kTXtvPSu"
      },
      "outputs": [],
      "source": [
        "print(df1.columns)\n",
        "# del df1[\"Unnamed: 0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xbl8YHJvPSu"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a72YUpSwvPSv"
      },
      "outputs": [],
      "source": [
        "df = df1[[\"A\",\"RA\",\"target\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCdXHHyiMrZc"
      },
      "source": [
        "from heere started the augmented sbert stratigie  \n",
        "\n",
        "---\n",
        "\n",
        "the Augmented SBERT (In-domain) strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc9Zq9aDMpJB",
        "outputId": "379aefef-26c0-4ef8-f9b7-cc3266218c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmBbprNYPOFP"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## *Train a Cross-Encoder*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "slD4RxntPIpb",
        "outputId": "64af1fb0-e532-40ab-b158-d9ac823eedf7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abed033d04a04bd48e2d665d9f9573e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf65c5e95618474dbf5692f397350c06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c63933d32cd04123ae6981790d63b412",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bafc461848f4ec6835be733e07d75c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70fb1fb82aba4246ac8b01cc14b7976d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7b58cb862c447bc9b702bc5a923635c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55c62e107b1d44398dc74b8bc83de596",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, InputExample\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a cross-encoder\n",
        "cross_encoder = CrossEncoder('bert-base-uncased', num_labels=len(df['target'].unique()))\n",
        "\n",
        "# Prepare the data\n",
        "gold_examples = [InputExample(texts=[row['A'], row['RA']], label=row['target']) for index, row in df.iterrows()]\n",
        "\n",
        "# Convert to DataLoader\n",
        "train_dataloader = DataLoader(gold_examples, shuffle=True, batch_size=16)\n",
        "\n",
        "# Train the cross-encoder\n",
        "cross_encoder.fit(train_dataloader, epochs=1, warmup_steps=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soaxygUaQYrk"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Initialize the T5 model and tokenizer\n",
        "model_name = 't5-small'  # You can choose a larger model for better results\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name).to('cuda')\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_paraphrases(sentence, num_paraphrases=1):\n",
        "    # T5 uses a task-specific prefix, e.g., \"paraphrase: \"\n",
        "    text = f\"paraphrase: {sentence} </s>\"\n",
        "    encoding = t5_tokenizer.encode_plus(text, padding=True, return_tensors=\"pt\")\n",
        "    input_ids, attention_masks = encoding[\"input_ids\"].to('cuda'), encoding[\"attention_mask\"].to('cuda')\n",
        "\n",
        "    # Generate paraphrases\n",
        "    outputs = t5_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_masks,\n",
        "        max_length=256,\n",
        "        num_return_sequences=num_paraphrases,\n",
        "        num_beams=10\n",
        "    )\n",
        "\n",
        "    # Decode paraphrases\n",
        "    paraphrases = [t5_tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in outputs]\n",
        "    return paraphrases\n",
        "\n",
        "# Use the function to generate paraphrases for each answer and reference answer\n",
        "augmented_examples = []\n",
        "for _, row in df.iterrows():\n",
        "    paraphrased_As = generate_paraphrases(row['A'])\n",
        "    paraphrased_RAs = row['RA']\n",
        "    for paraphrased_A in paraphrased_As:\n",
        "        for paraphrased_RA in paraphrased_RAs:\n",
        "            augmented_examples.append(InputExample(texts=[paraphrased_A, paraphrased_RA], label=row['target']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYi1fiAilM79",
        "outputId": "3e7b7eca-f643-419b-8ded-03105ad9c8e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.1.0+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.1.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk bert_score sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeZLRyLqwGgt"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from bert_score import score\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a semantic similarity model\n",
        "sim_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "def calculate_similarity(original, paraphrase):\n",
        "    original_emb = sim_model.encode(original, convert_to_tensor=True)\n",
        "    paraphrase_emb = sim_model.encode(paraphrase, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(original_emb, paraphrase_emb)\n",
        "    return cosine_scores.item()\n",
        "\n",
        "# Perform checks\n",
        "for example in augmented_examples:\n",
        "    original = example.texts[0]  # Assuming the first text is the original\n",
        "    paraphrase = example.texts[1]  # Assuming the second text is the paraphrase\n",
        "\n",
        "    print(f\"Original: {original}\")\n",
        "    print(f\"Paraphrase: {paraphrase}\")\n",
        "\n",
        "    # Check semantic similarity\n",
        "    similarity = calculate_similarity(original, paraphrase)\n",
        "    print(f\"Semantic Similarity: {similarity}\")\n",
        "\n",
        "    # Calculate BLEU score for diversity (inverted interpretation)\n",
        "    reference = [original.split()]\n",
        "    candidate = paraphrase.split()\n",
        "    bleu = sentence_bleu(reference, candidate)\n",
        "    print(f\"BLEU score (lower indicates more diversity): {bleu}\")\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    P, R, F1 = score([paraphrase], [original], lang=\"en\", verbose=False)\n",
        "    print(f\"BERTScore Precision: {P.item()}\")\n",
        "    print(f\"BERTScore Recall: {R.item()}\")\n",
        "    print(f\"BERTScore F1: {F1.item()}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E51fURzQf2H"
      },
      "source": [
        "Train a Bi-Encoder (SBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTtF3yVRQc6d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sentence_transformers import models, SentenceTransformer, losses\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the SentenceTransformer model with RoBERTa\n",
        "word_embedding_model = models.Transformer('roberta-base')\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "# Combine gold and silver data\n",
        "train_examples = gold_examples + augmented_examples  # Make sure these are defined\n",
        "\n",
        "# Train the model\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.SoftmaxLoss(\n",
        "    model=bi_encoder,\n",
        "    sentence_embedding_dimension=bi_encoder.get_sentence_embedding_dimension(),\n",
        "    num_labels=len(df['target'].unique())\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)], epochs=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9cJn1FWvJh2"
      },
      "outputs": [],
      "source": [
        "def batch_to_text(input_ids_batch, tokenizer):\n",
        "    texts = []\n",
        "    for input_ids in input_ids_batch:\n",
        "        # Remove padding token ids and convert to text\n",
        "        text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "        texts.append(text)\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPR0Nt8Yoftt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define a classification head module\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_classes):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.linear = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        logits = self.linear(features)\n",
        "        return logits\n",
        "\n",
        "# Assuming X_val and y_val are your validation datasets containing the input texts and labels\n",
        "X = df[['A', 'RA']]\n",
        "y = df['target']\n",
        "\n",
        "# Stratified split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "# Tokenize the validation dataset\n",
        "val_encodings = bi_encoder.tokenizer(X_val['A'].tolist(), X_val['RA'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Create a Dataset object from the tokenized texts and labels\n",
        "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(y_val.tolist()))\n",
        "\n",
        "# Create the DataLoader from the Dataset object\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize the classification head\n",
        "embedding_dim = bi_encoder.get_sentence_embedding_dimension()\n",
        "num_classes = len(df['target'].unique())\n",
        "classifier = ClassificationHead(embedding_dim, num_classes)\n",
        "\n",
        "# Move the classifier to the same device as the SentenceTransformer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classifier.to(device)\n",
        "bi_encoder.to(device)\n",
        "\n",
        "# Evaluate the combined model (SentenceTransformer + classification head)\n",
        "bi_encoder.eval()  # Set the SentenceTransformer to evaluation mode\n",
        "classifier.eval()  # Set the classification head to evaluation mode\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # Convert batch to text\n",
        "        texts = batch_to_text(input_ids.cpu(), bi_encoder.tokenizer)\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = bi_encoder.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n",
        "        embeddings = embeddings.to(device)\n",
        "\n",
        "        # Get the logits from the classifier\n",
        "        logits = classifier(embeddings)\n",
        "\n",
        "        # Get predictions from the logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Collect the true labels and predictions for evaluation\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "# Compute the accuracy and F1 score\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1_macro = f1_score(true_labels, predictions, average='macro')\n",
        "f1_weighted = f1_score(true_labels, predictions, average='weighted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwDoWnrlp7E-"
      },
      "source": [
        "### Prepare the Validation *Data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8RU7QP2Fvr3"
      },
      "source": [
        "### Calculate Accuracy and F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoMDA2fZFcxc"
      },
      "outputs": [],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ful24paR1_GP"
      },
      "outputs": [],
      "source": [
        "f1_macro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpKIFF5m2BBC"
      },
      "outputs": [],
      "source": [
        "f1_weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ0nL1dPDSfS"
      },
      "outputs": [],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ByHn-zMpsPO"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sentence_transformers import SentenceTransformer, InputExample\n",
        "# Initialize the T5 model and tokenizer\n",
        "model_name = 't5-small'  # You can choose a larger model for better results\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name).to('cuda')\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_paraphrases(sentence, num_paraphrases=1):\n",
        "    # T5 uses a task-specific prefix, e.g., \"paraphrase: \"\n",
        "    text = f\"paraphrase: {sentence} </s>\"\n",
        "    encoding = t5_tokenizer.encode_plus(text, padding=True, return_tensors=\"pt\")\n",
        "    input_ids, attention_masks = encoding[\"input_ids\"].to('cuda'), encoding[\"attention_mask\"].to('cuda')\n",
        "\n",
        "    # Generate paraphrases\n",
        "    outputs = t5_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_masks,\n",
        "        max_length=256,\n",
        "        num_return_sequences=num_paraphrases,\n",
        "        num_beams=10\n",
        "    )\n",
        "\n",
        "    # Decode paraphrases\n",
        "    paraphrases = [t5_tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in outputs]\n",
        "    return paraphrases\n",
        "\n",
        "# Use the function to generate paraphrases for each answer and reference answer\n",
        "augmented_examples = []\n",
        "for _, row in df.iterrows():\n",
        "    paraphrased_As = generate_paraphrases(row['A'])\n",
        "    paraphrased_RAs = row['RA']\n",
        "    for paraphrased_A in paraphrased_As:\n",
        "        for paraphrased_RA in paraphrased_RAs:\n",
        "            augmented_examples.append(InputExample(texts=[paraphrased_A, paraphrased_RA], label=row['target']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WQvDW9dluqfU",
        "outputId": "6b26ab27-a02e-410c-a50f-9c351c802298"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-b5b46a3bffbe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparaphrased_As\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'paraphrased_As' is not defined"
          ]
        }
      ],
      "source": [
        "paraphrased_As"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8icrslerFXOx"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Function for text augmentation using T5 model\n",
        "def t5_augment(text):\n",
        "    input_text = text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate augmented text\n",
        "    output_ids = t5_model.generate(input_ids, max_length=50, num_beams=5, length_penalty=1.0, early_stopping=True)\n",
        "    augmented_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return augmented_text\n",
        "\n",
        "# Apply the T5-based data augmentation function to the \"A\" column\n",
        "# df1['Augmented_A'] = df1['A'].apply(t5_augment)\n",
        "new_df = pd.DataFrame({\n",
        "    'A': df1['A'].apply(t5_augment),\n",
        "    'RA': df1['RA'],\n",
        "    'target': df1['target']\n",
        "})\n",
        "# # Create a new DataFrame with the augmented \"A\" and the original \"RA\"\n",
        "# new_df = pd.DataFrame({'A': df1['Augmented_A'], 'RA': df1['RA'], 'target': df1['target']})\n",
        "# # Reset the index for the new DataFrame\n",
        "# new_df = new_df.reset_index(drop=True)\n",
        "# new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8Tzog_GHvN-"
      },
      "outputs": [],
      "source": [
        "new_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK-cu4AcHozH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-T02f1f8h-i"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.concat([new_df, df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RakdDkKwazr"
      },
      "outputs": [],
      "source": [
        "combined_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD6mCgwfzhai"
      },
      "outputs": [],
      "source": [
        "combined_df[\"A\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLQM60wv0S-Y"
      },
      "outputs": [],
      "source": [
        "df = combined_df[[\"A\",\"RA\",\"target\"]]\n",
        "combined_df[\"RA\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV9qO1xivPSv"
      },
      "outputs": [],
      "source": [
        "batch_1 = df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y6XbggZ1UER"
      },
      "outputs": [],
      "source": [
        "batch_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOxIdyRCvPSv"
      },
      "outputs": [],
      "source": [
        "batch_1[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jakfP_U49i3z"
      },
      "outputs": [],
      "source": [
        "batch_1.isnull().sum()\n",
        "batch_1=batch_1.dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyPuFHt5yA2y"
      },
      "outputs": [],
      "source": [
        "batch_1.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EpKKqy8vPSv"
      },
      "outputs": [],
      "source": [
        "# For DistilBERT:\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "# tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "# model = model_class.from_pretrained(pretrained_weights)\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrNUKtBivPSw"
      },
      "source": [
        "Model #1: Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUxls237vPSw"
      },
      "source": [
        "Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n",
        "\n",
        "## Model #1: Preparing the Dataset\n",
        "Before we can hand our sentences to BERT, we need to do some minimal processing to put them in the format it requires.\n",
        "\n",
        "### Tokenization\n",
        "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8YfmXbm14lw"
      },
      "outputs": [],
      "source": [
        "inputs = [(a, ra) for a, ra in zip(batch_1[\"A\"], batch_1[\"RA\"])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEyC7DMwyp8B"
      },
      "outputs": [],
      "source": [
        "type(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ga6Lu6Q0kak"
      },
      "outputs": [],
      "source": [
        "inputs = [(a, ra) for a, ra in zip(batch_1[\"A\"], batch_1[\"RA\"])]\n",
        "# tokenizer(, , return_tensors='pt')\n",
        "encoded_inputs = tokenizer(inputs,\n",
        "                           padding=True,\n",
        "                           truncation=True,\n",
        "                           return_tensors='pt')\n",
        "encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnsd__o7vPSw"
      },
      "outputs": [],
      "source": [
        "# tokenized = batch_1[\"clean_text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "batch_1.reset_index(drop=True, inplace=True)\n",
        "tokenized = []\n",
        "for i in range(len(batch_1[\"A\"])):\n",
        "    # Tokenize the sentence pair\n",
        "    encoded = tokenizer.encode_plus(batch_1[\"A\"][i], batch_1[\"RA\"][i],\n",
        "                                     add_special_tokens=True,\n",
        "                                     max_length=512,\n",
        "                                     truncation=True,\n",
        "                                     padding='max_length',\n",
        "                                     return_attention_mask=True,\n",
        "                                     return_token_type_ids=True)\n",
        "    # Add the encoded sequence to the list\n",
        "    tokenized.append(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWvz0cbCvPSw"
      },
      "outputs": [],
      "source": [
        "# max_len = 0\n",
        "# for i in tokenized.values:\n",
        "#     if len(i) > max_len:\n",
        "#         max_len = len(i)\n",
        "max_len = 0\n",
        "for i in tokenized:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "padded = np.array([enc['input_ids'] + [0]*(max_len-len(enc['input_ids'])) for enc in tokenized])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X7-jz6b2izM"
      },
      "outputs": [],
      "source": [
        "padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qbJoTUjvPSw"
      },
      "outputs": [],
      "source": [
        "np.array(padded).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN1g56E_vPSx"
      },
      "source": [
        "### Masking\n",
        "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KezDBgiAvPSx"
      },
      "outputs": [],
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gqYdDGavPSx"
      },
      "outputs": [],
      "source": [
        "# input_ids = torch.tensor(padded)\n",
        "# attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZzgXNs80Mwj8"
      },
      "outputs": [],
      "source": [
        "# input_ids = torch.tensor(padded)\n",
        "# attention_mask = torch.tensor(attention_mask)\n",
        "# batch_size = 16\n",
        "# num_samples = len(input_ids)\n",
        "# last_hidden_states = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for i in range(0, num_samples, batch_size):\n",
        "#         input_ids_batch = torch.tensor(padded[i:i+batch_size])\n",
        "#         attention_mask_batch = torch.tensor(attention_mask[i:i+batch_size])\n",
        "#         batch_last_hidden_states = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
        "#         last_hidden_states.append(batch_last_hidden_states.last_hidden_state)\n",
        "\n",
        "# last_hidden_states = torch.cat(last_hidden_states, dim=0)\n",
        "input_ids = torch.tensor(padded)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "batch_size = 16\n",
        "num_samples = len(input_ids)\n",
        "last_hidden_states = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        input_ids_batch = torch.tensor(padded[i:i+batch_size])\n",
        "        attention_mask_batch = torch.tensor(attention_mask[i:i+batch_size])\n",
        "        batch_last_hidden_states = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
        "        last_hidden_states.append(batch_last_hidden_states.logits)\n",
        "\n",
        "last_hidden_states = torch.cat(last_hidden_states, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CFNyOJGtvPSx",
        "outputId": "fa3af08f-be26-4c62-8ff9-d2890ff756f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if len(last_hidden_states.shape) == 2:\n",
        "    features = last_hidden_states.numpy()\n",
        "else:\n",
        "    features = last_hidden_states[:, 0, :].numpy()\n",
        "\n",
        "#  features = last_hidden_states[:, 0, :].numpy()\n",
        "#  features = last_hidden_states[:, :].numpy()\n",
        "# last_hidden_states\n",
        "len(last_hidden_states.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VM2PGkO5vPSx",
        "outputId": "2d881c5d-4ab8-4a84-d2bd-f581f45b1531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test print here \n"
          ]
        }
      ],
      "source": [
        "labels = batch_1[\"target\"]\n",
        "print('test print here ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2jq0cBAzvPSy",
        "outputId": "32605946-3f61-4e57-b1ae-f8f1a7caf473"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.07821109, 0.08993697],\n",
              "       [0.08524515, 0.09736826],\n",
              "       [0.08686496, 0.07779203],\n",
              "       ...,\n",
              "       [0.10030397, 0.06955218],\n",
              "       [0.10056917, 0.07043003],\n",
              "       [0.08725807, 0.08558302]], dtype=float32)"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
        "train_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM0vP5UivPSy"
      },
      "source": [
        "We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. `LogisticRegression(C=5.2)`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "08kiOoB3vPSy"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_kP9VpTMvPSy",
        "outputId": "c045a600-0aeb-4b9c-8385-2c9e91e9b582"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr_clf = LogisticRegression()\n",
        "#lr_clf = DecisionTreeClassifier()\n",
        "lr_clf.fit(train_features, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L7su9S6uvPSy"
      },
      "outputs": [],
      "source": [
        "ypred = lr_clf.predict(test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nb40NlDCvPSz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I5NSlrqvvPSz",
        "outputId": "7f7e1717-eb17-4416-987d-9646d4a61da8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.583596214511041"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(ypred, test_labels,average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SAiRnEqhvPSz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PHJqoH9TueuS",
        "outputId": "edb8d5d3-263a-4728-a47c-aa3aa672dd6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      1.00      0.58       185\n",
            "           1       0.00      0.00      0.00       129\n",
            "           2       0.00      0.00      0.00       107\n",
            "           3       0.00      0.00      0.00        28\n",
            "\n",
            "    accuracy                           0.41       449\n",
            "   macro avg       0.10      0.25      0.15       449\n",
            "weighted avg       0.17      0.41      0.24       449\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels, ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vQs6eSQbfNIN"
      },
      "outputs": [],
      "source": [
        "dt_clf = DecisionTreeClassifier(max_depth=2)\n",
        "dt_clf.fit(train_features, train_labels)\n",
        "dt_predictions = dt_clf.predict(test_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mKUKrrarfhCa",
        "outputId": "f7b89ad9-faca-4dfc-9492-0517871dbfa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.41202672605790647\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      1.00      0.58       185\n",
            "           1       0.00      0.00      0.00       129\n",
            "           2       0.00      0.00      0.00       107\n",
            "           3       0.00      0.00      0.00        28\n",
            "\n",
            "    accuracy                           0.41       449\n",
            "   macro avg       0.10      0.25      0.15       449\n",
            "weighted avg       0.17      0.41      0.24       449\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(test_labels, dt_predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# You can also print a classification report for more detailed metrics\n",
        "print(classification_report(test_labels, dt_predictions))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HRpUotPUqEaq",
        "outputId": "6d97a785-d78c-4054-e2f0-2067cf176264"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.583596214511041"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(dt_predictions, test_labels,average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z8rwCV7GqlgE",
        "outputId": "9b99d8ea-d25a-49a5-a1ca-2475863d7c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.42316258351893093\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.97      0.58       185\n",
            "           1       0.00      0.00      0.00       129\n",
            "           2       0.52      0.10      0.17       107\n",
            "           3       0.00      0.00      0.00        28\n",
            "\n",
            "    accuracy                           0.42       449\n",
            "   macro avg       0.24      0.27      0.19       449\n",
            "weighted avg       0.30      0.42      0.28       449\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=3)\n",
        "rf_clf.fit(train_features, train_labels)\n",
        "rf_predictions = rf_clf.predict(test_features)\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(test_labels, rf_predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# You can also print a classification report for more detailed metrics\n",
        "print(classification_report(test_labels, rf_predictions))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rxj_k2Vqq1Zn",
        "outputId": "2e6ca57a-f478-41bc-986d-00798584e2c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5647371061121869"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(rf_predictions, test_labels,average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fB3Q51GMGMu6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create a VotingClassifier with your individual models\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('lr', lr_clf),  # Replace lr_clf with your trained Logistic Regression model\n",
        "    ('rf', rf_clf),  # Replace rf_clf with your trained Random Forest model\n",
        "    ('dt', dt_clf)   # Replace dt_clf with your trained DecisionTreeClassifier model\n",
        "], voting='hard')  # 'hard' for majority vote, or 'soft' for weighted vote\n",
        "\n",
        "# Fit the VotingClassifier on your training data (optional)\n",
        "voting_clf.fit(train_features, train_labels)\n",
        "ensemble_predictions = voting_clf.predict(test_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Argc5aSAHD-n",
        "outputId": "fabe4639-20e1-4128-a6cd-2af3ae8bffc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble Accuracy: 0.41202672605790647\n",
            "Ensemble F1-score: 0.24045723760477195\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "ensemble_accuracy = accuracy_score(test_labels, ensemble_predictions)\n",
        "ensemble_f1 = f1_score(test_labels, ensemble_predictions, average='weighted')\n",
        "\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
        "print(\"Ensemble F1-score:\", ensemble_f1)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}